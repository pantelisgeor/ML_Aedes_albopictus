{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "563e9313-3559-4edc-ba12-1777375d330d",
   "metadata": {},
   "source": [
    "## Machine learning ensemble modelling of $\\textit{Aedes albopictus}$ habitat suitability in the 21$^{21st}$ century\n",
    "### PLOS ONE 2021\n",
    "\n",
    "####  Dr. Pantelis Georgiades, Environmental Predictions Department, Climate & Atmosphere Research Centre, The Cyprus Institute  \n",
    "  \n",
    "This notebook can be used to download and construct the datasets needed for projecting the Aedes albopictus habitat suitability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328148a5-11aa-42f5-8eb5-8ff3c6ddc765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib\n",
    "import zipfile\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "import warnings\n",
    "from sys import argv\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06967a43-13e1-4a0f-a08d-fa4b03b12a33",
   "metadata": {},
   "source": [
    "# Land Use data\n",
    "\n",
    "### Ref: \n",
    "- Harmonization of global land use change and management for the period 850–2100 (LUH2) for CMIP6, Hurtt et al., Geosci. Model Dev., 13, 5425–5464, 2020\n",
    "https://doi.org/10.5194/gmd-13-5425-2020\n",
    "- GCAM-Demeter land use dataset at 0.05-degree resolution - https://dx.doi.org/10.25584/data.2020-07.1357/1644253    \n",
    "  \n",
    "  Download SSP2 RCP4.5 and SSP5 RCP8.5 GCAM-Harmonized (model means)   \n",
    "  (Data is freely avaibable but personal account and login needed to download)  \n",
    "\n",
    "  Files used :  \n",
    "  GCAM_Demeter_LU_H_ssp2_rcp45_modelmean_2015.nc -> GCAM_Demeter_LU_H_ssp2_rcp45_modelmean_2100.nc  \n",
    "  GCAM_Demeter_LU_H_ssp5_rcp85_modelmean_2015.nc -> GCAM_Demeter_LU_H_ssp5_rcp85_modelmean_2100.nc  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec53bf0-1a42-4dd6-83ed-080f22f98fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_land_use():\n",
    "    # Create the folders to hold the data if they don't exist\n",
    "    if not os.path.isdir('dat'):\n",
    "        os.mkdir('dat')\n",
    "    if not os.path.isdir('dat/land_use'):\n",
    "        os.mkdir('dat/land_use')\n",
    "    # Download the datasets\n",
    "    if not os.path.isfile('dat/land_use/multiple-states_input4MIPs_landState_ScenarioMIP_UofMD-MESSAGE-ssp245-2-1-f_gn_2015-2100.nc'):\n",
    "       urllib.request.urlretrieve('http://gsweb1vh2.umd.edu/LUH2/LUH2_v2f/MESSAGE/multiple-states_input4MIPs_landState_ScenarioMIP_UofMD-MESSAGE-ssp245-2-1-f_gn_2015-2100.nc',\n",
    "                                  'dat/land_use/multiple-states_input4MIPs_landState_ScenarioMIP_UofMD-MESSAGE-ssp245-2-1-f_gn_2015-2100.nc')\n",
    "    if not os.path.isfile('dat/land_use/multiple-states_input4MIPs_landState_ScenarioMIP_UofMD-MAGPIE-ssp585-2-1-f_gn_2015-2100.nc'):\n",
    "        urllib.request.urlretrieve('http://gsweb1vh2.umd.edu/LUH2/LUH2_v2f/MAGPIE/multiple-states_input4MIPs_landState_ScenarioMIP_UofMD-MAGPIE-ssp585-2-1-f_gn_2015-2100.nc',\n",
    "                                   'dat/land_use/multiple-states_input4MIPs_landState_ScenarioMIP_UofMD-MAGPIE-ssp585-2-1-f_gn_2015-2100.nc')\n",
    "    # Load the datasets\n",
    "    if not os.path.isfile('dat/land_use/luh_rcp45_monthly.nc'):\n",
    "        luh45 = xr\\\n",
    "            .open_dataset('dat/land_use/multiple-states_input4MIPs_landState_ScenarioMIP_' +\n",
    "                          'UofMD-MESSAGE-ssp245-2-1-f_gn_2015-2100.nc',\n",
    "                          decode_times=False) \\\n",
    "            .assign(time=pd.to_datetime([str(x) + '-12-31' for x in np.arange(2015, 2101, 1)]))\\\n",
    "            .expand_dims(scenario=['rcp45'])\\\n",
    "            .drop(['lat_bounds', 'lon_bounds', 'time_bnds'])\n",
    "        # Interpolate to a monthly temporal resolution\n",
    "        luh45 = luh45.interp(time = pd.date_range(start='2015-01-31', periods = 86*12, freq='M'),\n",
    "                             kwargs = {'fill_value': 'extrapolate'},\n",
    "                             method='nearest')\n",
    "        luh45.to_netcdf('dat/land_use/luh_rcp45_monthly.nc')\n",
    "    if not os.path.isfile('dat/luh_rcp85_monthly.nc'):\n",
    "        luh85 = xr \\\n",
    "            .open_dataset('dat/land_use/multiple-states_input4MIPs_landState_ScenarioMIP' +\n",
    "                          '_UofMD-MAGPIE-ssp585-2-1-f_gn_2015-2100.nc',\n",
    "                          decode_times=False) \\\n",
    "            .assign(time=pd.to_datetime([str(x) + '-12-31' for x in np.arange(2015, 2101, 1)]))\\\n",
    "            .expand_dims(scenario=['rcp85'])\\\n",
    "            .drop(['lat_bounds', 'lon_bounds', 'time_bnds'])\n",
    "        # Interpolate to a monthly temporal resolution\n",
    "        luh85 = luh85.interp(time = pd.date_range(start='2015-01-31', periods = 86*12, freq='M'),\n",
    "                             kwargs = {'fill_value': 'extrapolate'},\n",
    "                             method='nearest')\n",
    "        luh85.to_netcdf('dat/land_use/luh_rcp85_monthly.nc')\n",
    "        \n",
    "        \n",
    "def gcam_land_use():\n",
    "    path_gcam = 'dat/land_use_GCAM'\n",
    "    if not os.path.isfile(f'{path_gcam}/gcam.nc'):\n",
    "        files = sorted(os.listdir(path_gcam))\n",
    "        # Longitude and latitude ranges in the population density datasets\n",
    "        lon = np.arange(-179.875, 180, 0.25)\n",
    "        lat = np.arange(-89.875, 90, 0.25)\n",
    "        for file in files:\n",
    "            print(file)\n",
    "            scenario = file.split('_')[5]\n",
    "            year = file.split('_')[-1].split('.')[0]\n",
    "            file_temp = xr\\\n",
    "                .open_dataset(f'{path_gcam}/{file}')\\\n",
    "                .expand_dims(time=pd.to_datetime([f'{year}-01-31']), \n",
    "                             scenario=[scenario])\\\n",
    "                .rename({'longitude': 'lon', 'latitude': 'lat'})\n",
    "            # Interpolate to 0.25 degrees regular grid\n",
    "            file_temp = file_temp.interp(lon=lon, lat=lat, method='nearest')\n",
    "            if file == files[0]:\n",
    "                ds_gcam = file_temp\n",
    "            else:\n",
    "                ds_gcam = ds_gcam.merge(file_temp)\n",
    "            del file_temp, file\n",
    "        ds_gcam.to_netcdf(f'{path_gcam}/gcam.nc')\n",
    "    else:\n",
    "        ds_gcam = xr.open_dataset(f'{path_gcam}/gcam.nc')\n",
    "    # Interpolate to a monthly temporal resolution (each scenario separately to conserve memory\n",
    "    if not os.path.isfile(f'{path_gcam}/gcam_monthly_rcp45.nc'):\n",
    "        print('Interpolating RCP4.5 dataset')\n",
    "        gcam45 = ds_gcam.sel(scenario = 'rcp45', lat=slice(-54, 84)).drop('scenario')\n",
    "        gcam45 = gcam45.interp(time = pd.date_range('2015-01-31', periods = 86*12, freq='M'),\n",
    "                               kwargs = {'fill_value': 'extrapolate'},\n",
    "                               method = 'nearest')\n",
    "        print('Saving')\n",
    "        gcam45.to_netcdf(f'{path_gcam}/gcam_monthly_rcp45.nc')\n",
    "        del gcam45\n",
    "    if not os.path.isfile(f'{path_gcam}/gcam_monthly_rcp85.nc'):\n",
    "        print('Interpolating RCP8.5 dataset')\n",
    "        gcam85 = ds_gcam.sel(scenario = 'rcp85', lat=slice(-54, 84)).drop('scenario')\n",
    "        del ds_gcam\n",
    "        gcam85 = gcam85.interp(time = pd.date_range('2015-01-31', periods = 86*12, freq='M'),\n",
    "                               kwargs = {'fill_value': 'extrapolate'},\n",
    "                               method = 'nearest')\n",
    "        print('Saving')\n",
    "        gcam85.to_netcdf(f'{path_gcam}/gcam_monthly_rcp85.nc')\n",
    "        del gcam85"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb72e12d-32db-4365-a759-8bab4a16279e",
   "metadata": {},
   "source": [
    "# Population Density\n",
    "\n",
    "### Ref: \n",
    "\n",
    "Spatially explicit global population scenarios consistent with the Shared Socioeconomic Pathways, B Jones and B C O'Neill 2016 Environ. Res. Lett. 11 084003\n",
    "\n",
    "https://www.cgd.ucar.edu/iam/modeling/spatial-population-scenarios.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ecf71c-2ccd-4c41-b5b8-88f0d25c18d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pop_density():\n",
    "    # Create the folders to hold the data if they don't exist\n",
    "    if not os.path.isdir('dat'):\n",
    "        os.mkdir('dat')\n",
    "    if not os.path.isdir('dat/pop_density'):\n",
    "        os.mkdir('dat/pop_density')\n",
    "    if not os.path.isfile('dat/pop_density/pop_dens_rcp45.nc'):\n",
    "        # Download the datasets\n",
    "        if not os.path.isfile('Spatial_population_scenarios_NetCDF.zip'):\n",
    "            urllib.request.urlretrieve('https://www.cgd.ucar.edu/iam/modeling/data/Spatial_population_scenarios_NetCDF.zip',\n",
    "                                       'dat/pop_density/Spatial_population_scenarios_NetCDF.zip')\n",
    "            # Extract\n",
    "            with zipfile.ZipFile('dat/pop_density/Spatial_population_scenarios_NetCDF.zip') as zip_file:\n",
    "                zip_file.extractall('dat/pop_density')\n",
    "        # Read the datasets\n",
    "        path_pop_rcp45 = 'dat/pop_density/NetCDF/SSP2_NetCDF/total/NetCDF/'\n",
    "        path_pop_rcp85 = 'dat/pop_density/NetCDF/SSP5_NetCDF/total/NetCDF/'\n",
    "        # List the datasets in the two folders\n",
    "        files45 = sorted(os.listdir(path_pop_rcp45))\n",
    "        files85 = sorted(os.listdir(path_pop_rcp85))\n",
    "        # Longitude and latitude ranges in the population density datasets\n",
    "        lon = np.arange(-179.875, 180, 0.25)\n",
    "        lat = np.arange(-54.875, 84, 0.25)\n",
    "        for i in np.arange(len(files45)):\n",
    "            pop45 = xr\\\n",
    "                .open_dataset(f'{path_pop_rcp45}/{files45[i]}')\\\n",
    "                .expand_dims(time=pd.to_datetime([files45[i].split('_')[-1].split('.')[0] + '-01-31']))\\\n",
    "                .rename_vars({files45[i].replace('.nc', ''): 'pop_density'})\n",
    "            pop85 = xr\\\n",
    "                .open_dataset(f'{path_pop_rcp85}/{files85[i]}')\\\n",
    "                .expand_dims(time=pd.to_datetime([files85[i].split('_')[-1].split('.')[0] + '-01-31']))\\\n",
    "                .rename_vars({files85[i].replace('.nc', ''): 'pop_density'})\n",
    "            # Interpolate to 0.25 regular grid\n",
    "            pop45 = pop45.interp(lon=lon, lat=lat, method='nearest')\n",
    "            pop85 = pop85.interp(lon=lon, lat=lat, method='nearest')\n",
    "            if i == 0:\n",
    "                pop_dens_45 = pop45\n",
    "                pop_dens_85 = pop85\n",
    "            else:\n",
    "                pop_dens_45 = pop_dens_45.merge(pop45)\n",
    "                pop_dens_85 = pop_dens_85.merge(pop85)\n",
    "            del pop45, pop85\n",
    "        # Interpolate in time to monthly\n",
    "        pop_dens_45 = pop_dens_45.interp(time=pd.date_range(start='2010-01-31', periods=91*12, freq='M'), \n",
    "                                         kwargs={'fill_value': 'extrapolate'},\n",
    "                                         method='nearest')\n",
    "        pop_dens_85 = pop_dens_85.interp(time=pd.date_range(start='2010-01-31', periods=91*12, freq='M'), \n",
    "                                         kwargs={'fill_value': 'extrapolate'},\n",
    "                                         method='nearest')\n",
    "        pop_dens_45.to_netcdf('dat/pop_density/pop_dens_rcp45.nc')\n",
    "        pop_dens_85.to_netcdf('dat/pop_density/pop_dens_rcp85.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a750722f-0eca-4494-8807-10262bd63222",
   "metadata": {},
   "source": [
    "# NASA-NEX-GDDP\n",
    "## NASA Earth Exchange Global Daily Downscaled Climate Projections\n",
    "\n",
    "The provided function get_year() takes an xarray of NASA-NEX_GDDP data for a year, with daily temporal resolution and 0.25 degree spatial resolution. The xarrays variables are pr, tas, tasmin, tasmax (Precipitation, Mean temperature, Min temperature, Max temperature). It will aggregate the data to a monthly temporal resolution and calculate all the biologically relevant features used in the study.\n",
    "\n",
    "Ref:  \n",
    "Thrasher, B., Maurer, E. P., McKellar, C., & Duffy, P. B., 2012: Technical Note: Bias correcting climate model simulated daily temperature extremes with quantile mapping. Hydrology and Earth System Sciences, 16(9), 3309-3314. doi:10.5194/hess-16-3309-2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c32e6f-a05d-44da-9b76-072de37aacf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data for a year\n",
    "def get_year(ds):\n",
    "    # Resample the data monthly\n",
    "    ds = ds.resample(time='M').mean()\n",
    "    # Land/sea mask data\n",
    "    lsmask = xr\\\n",
    "        .open_dataset('dat/lsmask.nc')\n",
    "    # Add to the dataset\n",
    "    ds = ds.merge(lsmask)\n",
    "    del lsmask\n",
    "    # Convert the dataset to dataframe and select the land\n",
    "    df = ds.to_dataframe().reset_index(drop=False)\n",
    "    df = df.loc[df.lsmask == 0]\n",
    "    # Bio 1—Annual Mean Temperature\n",
    "    df_bio = df\\\n",
    "        .groupby(['lat', 'lon'], as_index=False)\\\n",
    "        .agg({'tas': np.mean})\\\n",
    "        .rename(columns={'tas': 'bio1'})\n",
    "    # Bio 2—Annual Mean Diurnal Range\n",
    "    df = df.assign(diurnal=df.tasmax-df.tasmin)\n",
    "    df_bio = pd.merge(df_bio, df.groupby(['lat', 'lon'], as_index=False)\n",
    "                      .agg({'diurnal': np.mean})\n",
    "                      .rename(columns={'diurnal': 'bio2'}),\n",
    "                      on=['lat', 'lon'], how='left')\n",
    "    # Bio 4—Temperature Seasonality(Standard Deviation)\n",
    "    # Bio 5—Max Temperature of Warmest Month\n",
    "    # Bio 6—Min Temperature of Coldest Month\n",
    "    df_bio = pd.merge(df_bio, df.groupby(['lat', 'lon'], as_index=False)\n",
    "                      .agg({'tas': np.std, 'tasmax': np.max, 'tasmin': np.min})\n",
    "                      .rename(columns={'tas': 'bio4', 'tasmax': 'bio5', 'tasmin': 'bio6'}),\n",
    "                      on=['lat', 'lon'], how='left')\n",
    "    # Bio 4a—Temperature Seasonality (CV)\n",
    "    df_bio = df_bio.assign(bio4a=100*(df_bio.bio4/df_bio.bio1))\n",
    "    # Bio 7—Annual Temperature Range\n",
    "    df_bio = df_bio.assign(bio7=df_bio.bio5-df_bio.bio6)\n",
    "    # Bio 8-11, 16-19\n",
    "    coords = df_bio[['lat', 'lon']].values\n",
    "    pool = mp.Pool(120)\n",
    "    func = partial(bio8_calc, df=df)\n",
    "    bio8to11 = pd.concat(pool.map(func, coords))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    bio8to11.reset_index(drop=True, inplace=True)\n",
    "    # Add to the rest\n",
    "    df_bio = pd.merge(df_bio, bio8to11, on=['lat', 'lon'], how='left')\n",
    "    del bio8to11, func\n",
    "    # Bio 12—Annual Precipitation\n",
    "    # Bio 13—Precipitation of Wettest Month\n",
    "    # Bio 14—Precipitation of Driest Month\n",
    "    df_temp = df.groupby(['lat', 'lon'])\\\n",
    "        .agg({'pr': [np.sum, np.max, np.min]})\\\n",
    "        .reset_index(drop=False, level=[0, 1])\n",
    "    df_temp.columns = [''.join(x) for x in df_temp.columns.values]\n",
    "    df_temp.rename(columns={'prsum': 'bio12', 'pramax': 'bio13', 'pramin': 'bio14'},\n",
    "                   inplace=True)\n",
    "    df_bio = pd.merge(df_bio, df_temp, on=['lat', 'lon'], how='left')\n",
    "    # Bio 15—Precipitation Seasonality (CV)\n",
    "    df_temp = df.groupby(['lat', 'lon'], as_index=False)\\\n",
    "        .agg({'pr': np.std})\\\n",
    "        .rename(columns={'pr': 'pr_std'})\n",
    "    df_temp = pd.merge(df_temp, df_bio[['lat', 'lon', 'bio12']],\n",
    "                       on=['lat', 'lon'], how='left')\n",
    "    df_temp = df_temp.assign(bio15=100*(df_temp.pr_std/(1+(df_temp.bio12/12))))\n",
    "    df_bio = pd.merge(df_bio, df_temp[['lat', 'lon', 'bio15']],\n",
    "                      on=['lat', 'lon'], how='left')\n",
    "    # Bio 3—Isothermality\n",
    "    df_bio = df_bio.assign(bio3=100*(df_bio.bio2/df_bio.bio7))\n",
    "    # Get all coordinates\n",
    "    coords = get_coords(sorted(df.time.unique()))\n",
    "    df_fin = pd.merge(coords, pd.merge(df, df_bio, on=['lat', 'lon'], how='left'),\n",
    "                      on=['lat', 'lon', 'time'], how='left')\n",
    "    ds_fin = df_fin.set_index(['lat', 'lon', 'time']).to_xarray()\n",
    "    return ds_fin\n",
    "\n",
    "\n",
    "def bio8_calc(coord, df):\n",
    "    # Get the data for this coordinates\n",
    "    df_temp = df\\\n",
    "        .loc[(df.lat == coord[0]) & (df.lon == coord[1])]\\\n",
    "        .reset_index(drop=True)\\\n",
    "        .assign(month=np.arange(1, 13, 1))\n",
    "    # Get the total precipitation for each quarter\n",
    "    df_q = pd.DataFrame()\n",
    "    for q in np.arange(1, 11, 1):\n",
    "        start = q\n",
    "        end = q+2\n",
    "        total_prec = df_temp\\\n",
    "            .loc[(df_temp.month >= start) & (df_temp.month <= end)].pr.sum()\n",
    "        total_tas = df_temp \\\n",
    "            .loc[(df_temp.month >= start) & (df_temp.month <= end)].tas.sum()\n",
    "        df_q = df_q.append(pd.DataFrame({'month_start': [start],\n",
    "                                         'month_end': [end],\n",
    "                                         'total_pr': [total_prec],\n",
    "                                         'total_tas': [total_tas]}))\n",
    "    # Get the mean temperature and total precipitation of the quarter\n",
    "    # with the highest precipitation\n",
    "    start, end = df_q\\\n",
    "        .loc[df_q.total_pr == df_q.total_pr.max()][['month_start', 'month_end']]\\\n",
    "        .values[0]\n",
    "    temp_q = df_temp\\\n",
    "        .loc[(df_temp.month >= start) & (df_temp.month <= end)]\\\n",
    "        .tas\\\n",
    "        .mean()\n",
    "    pr_high = df_temp\\\n",
    "        .loc[(df_temp.month >= start) & (df_temp.month <= end)]\\\n",
    "        .pr.sum()\n",
    "    # Get the mean temperature and total precipiation of the quarter\n",
    "    # with the lowest precipitation\n",
    "    start, end = df_q \\\n",
    "        .loc[df_q.total_pr == df_q.total_pr.min()][['month_start', 'month_end']] \\\n",
    "        .values[0]\n",
    "    temp_q_low = df_temp \\\n",
    "        .loc[(df_temp.month >= start) & (df_temp.month <= end)] \\\n",
    "        .tas \\\n",
    "        .mean()\n",
    "    pr_low = df_temp \\\n",
    "        .loc[(df_temp.month >= start) & (df_temp.month <= end)] \\\n",
    "        .pr.sum()\n",
    "    # Get the mean temperature of the hottest quarter\n",
    "    start, end = df_q \\\n",
    "        .loc[df_q.total_tas == df_q.total_tas.max()][['month_start', 'month_end']] \\\n",
    "        .values[0]\n",
    "    temp_max = df_temp \\\n",
    "        .loc[(df_temp.month >= start) & (df_temp.month <= end)] \\\n",
    "        .tas \\\n",
    "        .mean()\n",
    "    pr_temp_high = df_temp \\\n",
    "        .loc[(df_temp.month >= start) & (df_temp.month <= end)] \\\n",
    "        .pr.sum()\n",
    "    # Get the mean temperature of the coldest quarter\n",
    "    start, end = df_q \\\n",
    "        .loc[df_q.total_tas == df_q.total_tas.min()][['month_start', 'month_end']] \\\n",
    "        .values[0]\n",
    "    temp_low = df_temp \\\n",
    "        .loc[(df_temp.month >= start) & (df_temp.month <= end)] \\\n",
    "        .tas \\\n",
    "        .mean()\n",
    "    pr_temp_low = df_temp \\\n",
    "        .loc[(df_temp.month >= start) & (df_temp.month <= end)] \\\n",
    "        .pr.sum()\n",
    "    df_ret = pd.DataFrame({'lat': [coord[0]], 'lon': [coord[1]],\n",
    "                           'bio8': [temp_q], 'bio9': [temp_q_low],\n",
    "                           'bio10': [temp_max], 'bio11': [temp_low],\n",
    "                           'bio16': [pr_high], 'bio17': [pr_low],\n",
    "                           'bio18': [pr_temp_high], 'bio19': [pr_temp_low]})\n",
    "    return df_ret\n",
    "\n",
    "\n",
    "# Function to construct the coordinates for all globe\n",
    "def get_coords(dates):\n",
    "    lons = []\n",
    "    lats = []\n",
    "    time = []\n",
    "    for lon in np.arange(-179.875, 180, 0.25):\n",
    "        for lat in np.arange(-89.875, 90, 0.25):\n",
    "            for date in dates:\n",
    "                lons.append(lon)\n",
    "                lats.append(lat)\n",
    "                time.append(date)\n",
    "    return pd.DataFrame({'lat': lats, 'lon': lons, 'time': time})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0aa5927-ef8e-4f24-a94c-960cbff96025",
   "metadata": {},
   "source": [
    "### The following script can be used to construct a dataset for a specified climate model, year and scenario for predicting Aedes albopictus habitat suitability using the ensebmle machine learning model.\n",
    "\n",
    "An example dataset for the CNRM-CM5 climate model, scenario RCP8.5 for the year 2050 is provided in the dat folder.\n",
    "\n",
    "NOTE: The year is provided as YYYY (eg. 2021) and the scenario as 'rcp45' or 'rcp85'. The procedure for the climate and biologically relevant feature sets should be modified to reflect the available models downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d98a03-f95b-480e-aff7-d0cdfd6c975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_year_dataset(model, year, scenario='rcp45'):\n",
    "    # Read the LHU and GCAM land use datasets and select the year specified\n",
    "    ds_luh = xr.open_dataset(f'dat/land_use/luh_{scenario}_monthly.nc').sel(time=slice(f'{year}-01-31', f'{year}-12-31'))\n",
    "    ds_gcam = xr.open_dataset(f'dat/land_use_GCAM/gcam_monthly_{scenario}.nc').sel(time=slice(f'{year}-01-31', f'{year}-12-31'))\n",
    "    # Merge the two\n",
    "    ds = ds_gcam.merge(ds_luh)\n",
    "    ds = ds.drop('scenario')\n",
    "    del ds_luh, ds_gcam\n",
    "    # Read the population density dataset\n",
    "    ds_pop = xr.open_dataset(f'dat/pop_density/pop_dens_{scenario}.nc').sel(time=slice(f'{year}-01-31', f'{year}-12-31'))\n",
    "    # Add to the land use dataset\n",
    "    ds = ds.merge(ds_pop)\n",
    "    del ds_pop\n",
    "    # Read the climate and biovariables dataset\n",
    "    ds_clim = xr.open_dataset('dat/CCSM4_rcp85_2021.nc')  # <<-- This should be modified to reflect the location of the processed datasets\n",
    "    # Convert time variable to pandas datetime object\n",
    "    ds_clim = ds_clim.assign(time=pd.to_datetime([str(x).split(' ')[0] for x in ds_clim.time.values]))\n",
    "    # Merge with the rest\n",
    "    ds = ds.merge(ds_clim)\n",
    "    del ds_clim\n",
    "    # Add the land/sea mask (not used in the model, but used in the prediction procedures)\n",
    "    ds = ds.merge(xr.open_dataset('dat/lsmask.nc'))\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d71e0b-d7f2-47d4-97a3-b6471cf6efed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = predict_year_dataset(model='CNRM5-CM5', year=2050, scenario='rcp85')\n",
    "ds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
